{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRF3kD4B0DBW",
        "outputId": "3646d817-84ca-4ef4-c013-d2adb932f4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Iva Jorgusheska, 26/11/2024\n",
        "#UID: 11114620\n",
        "\n",
        "# === Import Libraries ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM,  LayerNormalization, Bidirectional,BatchNormalization, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Sequential, load_model\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "# === Load Data ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/validation.csv')\n",
        "test_data = pd.read_csv('/content/CW2-test-dataset.csv', header = None)\n",
        "# Genre columns for multi-label classification\n",
        "genre_columns = ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzEbQRpf0YYh",
        "outputId": "5f50a2ce-2e01-42d1-93f2-2dbd922979f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    young girl bird aunt boyfriend waiting car vis...\n",
            "1    oliver pease burgess meredith deceived bride m...\n",
            "2    ann sarah polley mother two small daughters un...\n",
            "3    ben jamie brett gabel arthur sam mraovich gay ...\n",
            "4    new york city 16th precinct police detective d...\n",
            "Name: processed_plot, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# === Preprocess Text ===\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return ' '.join(token for token in tokens if token.isalnum() and (token not in stop_words or token in ['not', 'no']))\n",
        "\n",
        "train_data['processed_plot'] = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "val_data['processed_plot'] = val_data.iloc[:, 2].apply(preprocess_text)\n",
        "#test_data['processed_plot'] = test_data['plot_synopsis'].apply(preprocess_text)\n",
        "test_data['processed_plot'] = test_data.iloc[:, 2].apply(preprocess_text)\n",
        "print(test_data['processed_plot'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_data['processed_plot'][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf7g5Suubbfz",
        "outputId": "3256f451-7d75-4d39-c9b5-d740a190ec35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    enchanting city verona italy renowned painter ...\n",
            "1    walker works friend mal reese steal large amou...\n",
            "2    film consists several thematically linked scen...\n",
            "3    gentleman dignity careers love lives urban pro...\n",
            "4    carmen brown beyonc√© seductive aspiring actres...\n",
            "Name: processed_plot, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LBbo0DEh0d93"
      },
      "outputs": [],
      "source": [
        "# === Tokenize and Pad Text ===\n",
        "vocab_size = 30000\n",
        "max_length = 600\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size + 1, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_data['processed_plot'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['processed_plot'])\n",
        "val_sequences = tokenizer.texts_to_sequences(val_data['processed_plot'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['processed_plot'])\n",
        "\n",
        "padded_train_texts = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_val_texts = pad_sequences(val_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "padded_test_texts = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "03V9Ka7X0gRe"
      },
      "outputs": [],
      "source": [
        "# Extract labels\n",
        "train_labels = train_data[genre_columns].values\n",
        "val_labels = val_data[genre_columns].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzYAJNKx0mJw",
        "outputId": "8f07d808-5637-4c30-8e73-17551d69d147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: {0: 0.7269765803838704, 1: 0.5094083533839225, 2: 0.5, 3: 4.932497013142174, 4: 0.5, 5: 0.5536779990612217, 6: 0.5, 7: 4.497276688453159, 8: 0.5}\n"
          ]
        }
      ],
      "source": [
        "# === Dynamic Class Weights ===\n",
        "label_counts = train_labels.sum(axis=0)\n",
        "total_samples = len(train_labels)\n",
        "#calculate weights to make up for undersamled/oversampled classes\n",
        "class_weights = {\n",
        "    i: min(max(total_samples / (len(label_counts) * count), 0.5), 5.0)\n",
        "    for i, count in enumerate(label_counts)\n",
        "}\n",
        "print(f\"Class Weights: {class_weights}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C5rQRPba0uhY"
      },
      "outputs": [],
      "source": [
        "# === Learning Rate Scheduler ===\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-4, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hv_rZr8wHhys"
      },
      "outputs": [],
      "source": [
        "# === Load Pre-trained Word Embeddings ===\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sKSzIaHBDVSV"
      },
      "outputs": [],
      "source": [
        "# === Focal Loss Function ===\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        return -tf.reduce_mean(alpha * (1 - pt) ** gamma * tf.math.log(pt + tf.keras.backend.epsilon()))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MNqEv7CU00ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6e6975-17f2-4b6b-ee0d-aabd3f53acfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# === Step 6: Build the Enhanced Model ===\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Attention, Input, GlobalAveragePooling1D, Concatenate\n",
        "\n",
        "# === Build Model with Attention ===\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, Bidirectional, LSTM, Attention, Dense, Dropout, BatchNormalization,\n",
        "    LayerNormalization, GlobalAveragePooling1D, Input\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_hybrid_model(vocab_size, embedding_dim, embedding_matrix, max_length, num_classes,\n",
        "                       lstm_units=128, dropout_rate=0.2, learning_rate=1e-4, use_focal_loss=False):\n",
        "    \"\"\"\n",
        "    Hybrid model combining stacked LSTMs with attention mechanism.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size: Vocabulary size\n",
        "    - embedding_dim: Dimensionality of embedding space\n",
        "    - embedding_matrix: Pre-trained embedding weights\n",
        "    - max_length: Max sequence length\n",
        "    - num_classes: Number of output classes (multi-label)\n",
        "    - lstm_units: Number of units in each LSTM layer\n",
        "    - dropout_rate: Dropout rate for regularization\n",
        "    - learning_rate: Learning rate for optimizer\n",
        "    - use_focal_loss: If True, use focal loss; else use binary cross-entropy\n",
        "\n",
        "    Returns:\n",
        "    - Compiled Keras model\n",
        "    \"\"\"\n",
        "    # === Input Layer ===\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "\n",
        "    # === Embedding Layer ===\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size + 1,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_length,\n",
        "        trainable=True  # Allow fine-tuning\n",
        "    )(input_layer)\n",
        "\n",
        "    # === Stacked Bidirectional LSTMs ===\n",
        "    lstm_1 = Bidirectional(LSTM(lstm_units//2, return_sequences=True))(embedding_layer)\n",
        "    lstm_1 = BatchNormalization()(lstm_1)\n",
        "    lstm_1 = Dropout(dropout_rate)(lstm_1)\n",
        "\n",
        "    lstm_2 = Bidirectional(LSTM(lstm_units//4, return_sequences=True))(lstm_1)\n",
        "    lstm_2 = LayerNormalization()(lstm_2)\n",
        "    lstm_2 = Dropout(dropout_rate)(lstm_2)\n",
        "\n",
        "    # === Attention Mechanism ===\n",
        "    attention_output = Attention()([lstm_2, lstm_2])\n",
        "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "    # === Fully Connected Layers ===\n",
        "    dense = Dense(lstm_units // 2, activation='relu', kernel_regularizer='l2')(pooled_output)\n",
        "    dropout = Dropout(dropout_rate)(dense)\n",
        "    output_layer = Dense(num_classes, activation='sigmoid')(dropout)\n",
        "\n",
        "    # === Compile Model ===\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Select loss function dynamically\n",
        "    loss_function = (\n",
        "        focal_loss(gamma=2.0) if use_focal_loss else \"binary_crossentropy\"\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=loss_function,\n",
        "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_hybrid_model(vocab_size, embedding_dim, embedding_matrix, max_length, 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s3zRasex06Mf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55ac7dc-b31e-42bd-8901-3179c1a92ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 96ms/step - accuracy: 0.0948 - loss: 0.9444 - precision: 0.2434 - recall: 0.1281 - val_accuracy: 0.2045 - val_loss: 0.9424 - val_precision: 0.7211 - val_recall: 0.0787 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 90ms/step - accuracy: 0.2582 - loss: 0.7237 - precision: 0.6126 - recall: 0.1248 - val_accuracy: 0.3148 - val_loss: 0.7761 - val_precision: 0.7195 - val_recall: 0.1718 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.2766 - loss: 0.6045 - precision: 0.6441 - recall: 0.1861 - val_accuracy: 0.3224 - val_loss: 0.6732 - val_precision: 0.6938 - val_recall: 0.2336 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.2932 - loss: 0.5077 - precision: 0.6579 - recall: 0.2327 - val_accuracy: 0.3241 - val_loss: 0.6026 - val_precision: 0.6755 - val_recall: 0.2671 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - accuracy: 0.3082 - loss: 0.4426 - precision: 0.6537 - recall: 0.2532 - val_accuracy: 0.3249 - val_loss: 0.5482 - val_precision: 0.6866 - val_recall: 0.2897 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - accuracy: 0.3202 - loss: 0.3904 - precision: 0.6833 - recall: 0.2746 - val_accuracy: 0.3258 - val_loss: 0.5167 - val_precision: 0.7238 - val_recall: 0.2371 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - accuracy: 0.3177 - loss: 0.3559 - precision: 0.6966 - recall: 0.2771 - val_accuracy: 0.3215 - val_loss: 0.4906 - val_precision: 0.6827 - val_recall: 0.2592 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 97ms/step - accuracy: 0.3284 - loss: 0.3289 - precision: 0.6801 - recall: 0.2927 - val_accuracy: 0.3316 - val_loss: 0.4660 - val_precision: 0.6631 - val_recall: 0.3210 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 89ms/step - accuracy: 0.3343 - loss: 0.3066 - precision: 0.6975 - recall: 0.3214 - val_accuracy: 0.3325 - val_loss: 0.4575 - val_precision: 0.6454 - val_recall: 0.3175 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - accuracy: 0.3285 - loss: 0.2892 - precision: 0.6984 - recall: 0.3231 - val_accuracy: 0.3274 - val_loss: 0.4459 - val_precision: 0.6346 - val_recall: 0.3497 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ],
      "source": [
        "# === Train Model with Callbacks ===\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-4, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    padded_train_texts,\n",
        "    train_labels,\n",
        "    validation_data=(padded_val_texts, val_labels),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g75lkIdR070X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5940c9c-3927-4e32-fa86-effb529a1429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "#Tune threshold for each class\n",
        "def tune_individual_thresholds(y_true, y_pred_prob):\n",
        "    optimal_thresholds = []\n",
        "    for i in range(y_pred_prob.shape[1]):\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_prob[:, i])\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "        optimal_thresholds.append(best_threshold)\n",
        "    return np.clip(optimal_thresholds, 0.1, 0.9)  # Cap thresholds for stability\n",
        "\n",
        "val_predictions_prob = model.predict(padded_val_texts)\n",
        "optimal_thresholds = tune_individual_thresholds(val_labels, val_predictions_prob)\n",
        "\n",
        "# Use these thresholds in prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k2GxNsbL2ZVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942af352-68c8-4965-f9aa-7c7fd3767e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
            "Predictions saved to /content/val_predictions.csv\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "Predictions saved to /content/drive/MyDrive/test_predictions.csv\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Predictions saved to /content/test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# === Predict and Save Results ===\n",
        "def predict_and_save_results(model, val_padded, val_data, genre_columns, thresholds, output_file='validation_predictions.csv'):\n",
        "    predictions_prob = model.predict(val_padded)\n",
        "    predictions_binary = (predictions_prob > thresholds).astype(int)\n",
        "    result = pd.DataFrame({'ID': val_data.iloc[:, 0]})\n",
        "    result[genre_columns] = predictions_binary\n",
        "    result.to_csv(output_file, index=False, header=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "\n",
        "# def predict_and_save_results_test(model, test_padded, test_data, thresholds, output_file):\n",
        "#     predictions_prob = model.predict(test_padded)\n",
        "#     predictions_binary = (predictions_prob > thresholds).astype(int)\n",
        "\n",
        "#     # Create a DataFrame for the results\n",
        "#     result = pd.DataFrame(test_data.iloc[:, 0])  # Keep only the ID column from input\n",
        "\n",
        "#     # Add columns for predictions\n",
        "#     for i in range(predictions_binary.shape[1]):\n",
        "#         result[f'genre_{i+1}'] = predictions_binary[:, i]\n",
        "\n",
        "\n",
        "#     # Save the DataFrame\n",
        "#     result.to_csv(output_file, index=False, header=False)\n",
        "#     print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "predict_and_save_results(\n",
        "    model,\n",
        "    padded_val_texts,\n",
        "    val_data,\n",
        "    genre_columns,\n",
        "    optimal_thresholds,\n",
        "    output_file='/content/val_predictions.csv'\n",
        ")\n",
        "\n",
        "predict_and_save_results(\n",
        "    model,\n",
        "    padded_test_texts,\n",
        "    test_data,\n",
        "    genre_columns,\n",
        "    optimal_thresholds,\n",
        "    output_file='/content/drive/MyDrive/test_predictions.csv'\n",
        ")\n",
        "predict_and_save_results(\n",
        "    model,\n",
        "    padded_test_texts,\n",
        "    test_data,\n",
        "    genre_columns,\n",
        "    optimal_thresholds,\n",
        "    output_file='/content/test_predictions.csv'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}