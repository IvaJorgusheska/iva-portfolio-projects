{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**An innovative solution inspired by state-of-the-art research: We fine-tuned three transformers and built a meta-learner on top to aggregate their probability predictions and determine the final class.**"
      ],
      "metadata": {
        "id": "mCh4FLR7KPyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPaMN8ogT2fj"
      },
      "outputs": [],
      "source": [
        "#Necessary imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "#Empty the cuda cache to be sure we have enough space to load the transformer models\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting to google drive to be able to save and load the models there\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models_nli\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TmiX-rX4uD-",
        "outputId": "517e6ddb-7261-4921-d1e3-b37ed3c0cf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing the data**"
      ],
      "metadata": {
        "id": "NswlRm9sQzQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text: normalize spaces, remove misplaced punctuation, fix contractions.\"\"\"\n",
        "    text = str(text).strip().lower()\n",
        "\n",
        "    # Fix spaces around punctuation (keep punctuation but standardize spacing)\n",
        "    text = re.sub(r'\\s+([?.!,\"])', r'\\1', text)  # Removes spaces before punctuation\n",
        "    text = re.sub(r'([?.!,\"])', r'\\1 ', text)  # Ensures one space after punctuation\n",
        "\n",
        "    # Normalize quotes (remove extra surrounding quotes)\n",
        "    text = re.sub(r'^\"|\"$', '', text)\n",
        "\n",
        "    # Handle common contractions\n",
        "    text = re.sub(r\"\\bd'you\\b\", \"do you\", text)\n",
        "    text = re.sub(r\"\\b'cause\\b\", \"because\", text)\n",
        "    text = re.sub(r\"\\bi'm\\b\", \"i am\", text)\n",
        "    text = re.sub(r\"\\bain't\\b\", \"is not\", text)\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "9xTKULkv4uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train set for finetunign the transformers\n",
        "# Load training dataset\n",
        "df_train = pd.read_csv(\"train.csv\", quotechar='\"', delimiter=\",\", encoding=\"utf-8\")\n",
        "df_train.columns = [\"premise\", \"hypothesis\", \"label\"]  # Ensure correct column names\n",
        "df_train.dropna(inplace=True)  # Remove missing values\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXj0x-Kx4yRr",
        "outputId": "0b9cd50a-9638-476c-8375-6a4d9b07e032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             premise  \\\n",
            "0  yeah i don't know cut California in half or so...   \n",
            "1                      actual names will not be used   \n",
            "2          The film was directed by Randall Wallace.   \n",
            "3   \"How d'you know he'll sign me on?\"Anse studie...   \n",
            "4  In the light of the candles his cheeks looked ...   \n",
            "\n",
            "                                          hypothesis  label  \n",
            "0  Yeah. I'm not sure how to make that fit. Maybe...      1  \n",
            "1  For the sake of privacy, actual names are not ...      1  \n",
            "2  The film was directed by Randall Wallace and s...      1  \n",
            "3       Anse looked at himself in a cracked mirror.       1  \n",
            "4  Drew regarded his best friend and noted that i...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text cleaning\n",
        "df_train[\"premise\"] = df_train[\"premise\"].apply(clean_text)\n",
        "df_train[\"hypothesis\"] = df_train[\"hypothesis\"].apply(clean_text)\n",
        "\n",
        "df_train[\"label\"] = df_train[\"label\"].astype(int)\n",
        "\n",
        "# Print first few samples\n",
        "print(\"Training Dataset Sample:\")\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfiWbd94yrH",
        "outputId": "9e215846-ad9f-4f99-9fc0-82aaec8c5155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Sample:\n",
            "                                             premise  \\\n",
            "0  yeah i don't know cut california in half or so...   \n",
            "1                      actual names will not be used   \n",
            "2         the film was directed by randall wallace.    \n",
            "3   how do you know he'll sign me on? \" anse stud...   \n",
            "4  in the light of the candles his cheeks looked ...   \n",
            "\n",
            "                                          hypothesis  label  \n",
            "0  yeah.  i am not sure how to make that fit.  ma...      1  \n",
            "1  for the sake of privacy,  actual names are not...      1  \n",
            "2  the film was directed by randall wallace and s...      1  \n",
            "3       anse looked at himself in a cracked mirror.       1  \n",
            "4  drew regarded his best friend and noted that i...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation set for finetuning the transformers\n",
        "# Load validation dataset\n",
        "df_val = pd.read_csv(\"dev.csv\", quotechar='\"', delimiter=\",\", encoding=\"utf-8\")\n",
        "df_val.columns = [\"premise\", \"hypothesis\", \"label\"]  # Ensure correct column names\n",
        "df_val.dropna(inplace=True)  # Remove missing values\n",
        "\n",
        "# Apply text cleaning\n",
        "df_val[\"premise\"] = df_val[\"premise\"].apply(clean_text)\n",
        "df_val[\"hypothesis\"] = df_val[\"hypothesis\"].apply(clean_text)\n",
        "\n",
        "df_val[\"label\"] = df_val[\"label\"].astype(int)\n",
        "\n",
        "# Print first few samples\n",
        "print(\"Validation Dataset Sample:\")\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YukcPRB5415K",
        "outputId": "97003fcb-2419-4f23-9dfc-a66fac04e59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Dataset Sample:\n",
            "                                             premise  \\\n",
            "0  by starting at the soft underbelly,  the 16, 0...   \n",
            "1  the class had broken into a light sweat,  but ...   \n",
            "2  samson had his famous haircut here,  but he wo...   \n",
            "3  a man with a black shirt holds a baby while a ...   \n",
            "4  i know that many of you are interested in addr...   \n",
            "\n",
            "                                          hypothesis  label  \n",
            "0  general nelson a.  miles had 30, 000 troops in...      0  \n",
            "1        the class grew more tense as time went on.       1  \n",
            "2  it was unknown where exactly within the town s...      1  \n",
            "3  a darkly dressed man passes a crying baby to a...      0  \n",
            "4                     the problems must be addressed      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validation set for the meta learner\n",
        "# Split into 80% training and 20% validation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_meta_train, df_meta_val = train_test_split(df_val, test_size=0.2, random_state=42, stratify=df_val[\"label\"])\n"
      ],
      "metadata": {
        "id": "02qHufnu437j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset creation function and loading the needed tokenizers**"
      ],
      "metadata": {
        "id": "yTiaHU7sQ7bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NliDataset(Dataset):\n",
        "  \"\"\"Dataset for the NLI task\"\"\"\n",
        "  def __init__(self, premises, hypotheses, labels, tokenizer, max_lenth = 124):\n",
        "    self.premises = premises\n",
        "    self.hypotheses = hypotheses\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_lenth\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.premises)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      encoding = self.tokenizer(self.premises[idx], self.hypotheses[idx], padding='max_length',truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "      return {\n",
        "          'input_ids': encoding['input_ids'].squeeze(0),\n",
        "          'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "          'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "      }\n",
        "\n",
        "# Initialize tokenizers for different models\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "# deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "albert_tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ePRefXp45jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**-----------------Transformers finetuning---------------------------**"
      ],
      "metadata": {
        "id": "kFBNJ6jdRBSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create datasets for the transformers\n",
        "bert_t_train_dataset = NliDataset(df_train[\"premise\"].tolist(), df_train[\"hypothesis\"].tolist(), df_train[\"label\"].tolist(), bert_tokenizer)\n",
        "bert_t_val_dataset = NliDataset(df_val[\"premise\"].tolist(), df_val[\"hypothesis\"].tolist(), df_val[\"label\"].tolist(), bert_tokenizer)\n",
        "\n",
        "roberta_t_train_dataset = NliDataset(df_train[\"premise\"].tolist(), df_train[\"hypothesis\"].tolist(), df_train[\"label\"].tolist(), roberta_tokenizer)\n",
        "roberta_t_val_dataset = NliDataset(df_val[\"premise\"].tolist(), df_val[\"hypothesis\"].tolist(), df_val[\"label\"].tolist(), roberta_tokenizer)\n",
        "\n",
        "\n",
        "# deberta_train_dataset = NliDataset(df_train[\"premise\"].tolist(), df_train[\"hypothesis\"].tolist(), df_train[\"label\"].tolist(), deberta_tokenizer)\n",
        "# deberta_val_dataset = NliDataset(df_val[\"premise\"].tolist(), df_val[\"hypothesis\"].tolist(), df_val[\"label\"].tolist(), deberta_tokenizer)\n",
        "\n",
        "albert_t_train_dataset = NliDataset(df_train[\"premise\"].tolist(), df_train[\"hypothesis\"].tolist(), df_train[\"label\"].tolist(), albert_tokenizer)\n",
        "albert_t_val_dataset = NliDataset(df_val[\"premise\"].tolist(), df_val[\"hypothesis\"].tolist(), df_val[\"label\"].tolist(), albert_tokenizer)\n",
        "\n",
        "\n",
        "# Create DataLoaders for the transformers\n",
        "bert_t_train_loader = DataLoader(bert_t_train_dataset, batch_size=64, shuffle=False)\n",
        "bert_t_val_loader = DataLoader(bert_t_val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "roberta_t_train_loader = DataLoader(roberta_t_train_dataset, batch_size=64, shuffle=False)\n",
        "roberta_t_val_loader = DataLoader(roberta_t_val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# deberta_train_loader = DataLoader(deberta_train_dataset, batch_size=64, shuffle=True)\n",
        "# deberta_val_loader = DataLoader(deberta_val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "albert_t_train_loader = DataLoader(albert_t_train_dataset, batch_size=64, shuffle=False)\n",
        "albert_t_val_loader = DataLoader(albert_t_val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "eG8IBjoTNDQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_transformer(model_name, train_loader, val_loader, epochs=3, learning_rate=2e-5, use_focal_loss=False, patience=3):\n",
        "    \"\"\"Train a transformer model:\n",
        "       - Gradient Clipping\n",
        "       - Learning Rate Scheduler\n",
        "       - Early Stopping\n",
        "       - Weight Decay (Regularization)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss function\n",
        "    if use_focal_loss:\n",
        "        class FocalLoss(nn.Module):\n",
        "            def __init__(self, gamma=2.0, alpha=0.25):\n",
        "                super(FocalLoss, self).__init__()\n",
        "                self.gamma = gamma\n",
        "                self.alpha = alpha\n",
        "\n",
        "            def forward(self, inputs, targets):\n",
        "                ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
        "                pt = torch.exp(-ce_loss)\n",
        "                focal_loss = (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
        "                return focal_loss\n",
        "\n",
        "        criterion = FocalLoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer with Weight Decay for regularization\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    # Learning Rate Scheduler\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n",
        "    )\n",
        "\n",
        "    # Early stopping setup\n",
        "    best_val_loss = float(\"inf\")\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()  # Update learning rate\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        print(f\"Epoch {epoch+1} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            early_stop_counter = 0\n",
        "            torch.save(model.state_dict(), f\"{model_name}_best.pth\")  # Save the best model\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} consecutive epochs.\")\n",
        "                break\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "9ZrXAciI47gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WE RUN THIS CELL ONLY TO TRAIN THE TRANSFORMERS, ONCE WE HAVE TRAINED THEM WE RUN THE NEXT CELL JUST TO LOAD THEM FROM DRIVE\n",
        "\n",
        "#Fine-tuning the transformers by calling our function and saving the models on the drive\n",
        "bert_model = train_transformer(\"bert-base-uncased\", bert_t_train_loader, bert_t_val_loader)\n",
        "torch.save(bert_model.state_dict(), f\"{SAVE_PATH}/bert_finetuned.pth\")\n",
        "roberta_model = train_transformer(\"roberta-base\", roberta_t_train_loader, roberta_t_val_loader)\n",
        "torch.save(roberta_model.state_dict(), f\"{SAVE_PATH}/roberta_finetuned.pth\")\n",
        "\n",
        "# deberta_model = train_transformer(\"microsoft/deberta-v3-base\", deberta_train_loader, deberta_val_loader, use_focal_loss=True)\n",
        "# torch.save(deberta_model.state_dict(), f\"{SAVE_PATH}/deberta_finetuned.pth\")\n",
        "\n",
        "albert_model = train_transformer(\"albert-base-v2\", albert_t_train_loader, albert_t_val_loader, use_focal_loss=True)\n",
        "torch.save(albert_model.state_dict(), f\"{SAVE_PATH}/albert_finetuned.pth\")"
      ],
      "metadata": {
        "id": "XiQgsMP64933"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOADS THE FINETUNED TRANSFORMERS\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models_nli\"\n",
        "\n",
        "def load_finetuned_model(model_name, path):\n",
        "    \"\"\"Load a fine-tuned transformer model from saved weights.\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2-class NLI task\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "bert_model = load_finetuned_model(\"bert-base-uncased\", f\"{SAVE_PATH}/bert_finetuned.pth\")\n",
        "roberta_model = load_finetuned_model(\"roberta-base\", f\"{SAVE_PATH}/roberta_finetuned.pth\")\n",
        "albert_model = load_finetuned_model(\"albert-base-v2\", f\"{SAVE_PATH}/albert_finetuned.pth\")\n"
      ],
      "metadata": {
        "id": "xhu0QJQnQleV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**------------Meta learner------------**"
      ],
      "metadata": {
        "id": "ykcufifTRXvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definition of the meta model\n",
        "import torch.nn as nn\n",
        "\n",
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, hidden_size=128, dropout=0.2):\n",
        "        super(MetaLearner, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),  # First layer\n",
        "            nn.ReLU(),  # Activation function\n",
        "            nn.Dropout(dropout),  # Regularization\n",
        "            nn.Linear(hidden_size, hidden_size // 2),  # Second layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, num_classes)  # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "A_S1n3fU5Ffg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create datasets for the meta learner\n",
        "bert_meta_train_dataset = NliDataset(df_meta_train[\"premise\"].tolist(), df_meta_train[\"hypothesis\"].tolist(), df_meta_train[\"label\"].tolist(), bert_tokenizer)\n",
        "bert_meta_val_dataset = NliDataset(df_meta_val[\"premise\"].tolist(), df_meta_val[\"hypothesis\"].tolist(), df_meta_val[\"label\"].tolist(), bert_tokenizer)\n",
        "\n",
        "roberta_meta_train_dataset = NliDataset(df_meta_train[\"premise\"].tolist(), df_meta_train[\"hypothesis\"].tolist(), df_meta_train[\"label\"].tolist(), roberta_tokenizer)\n",
        "roberta_meta_val_dataset = NliDataset(df_meta_val[\"premise\"].tolist(), df_meta_val[\"hypothesis\"].tolist(), df_meta_val[\"label\"].tolist(), roberta_tokenizer)\n",
        "\n",
        "# deberta_train_dataset = NliDataset(df_train[\"premise\"].tolist(), df_train[\"hypothesis\"].tolist(), df_train[\"label\"].tolist(), deberta_tokenizer)\n",
        "# deberta_val_dataset = NliDataset(df_val[\"premise\"].tolist(), df_val[\"hypothesis\"].tolist(), df_val[\"label\"].tolist(), deberta_tokenizer)\n",
        "\n",
        "albert_meta_train_dataset = NliDataset(df_meta_train[\"premise\"].tolist(), df_meta_train[\"hypothesis\"].tolist(), df_meta_train[\"label\"].tolist(), albert_tokenizer)\n",
        "albert_meta_val_dataset = NliDataset(df_meta_val[\"premise\"].tolist(), df_meta_val[\"hypothesis\"].tolist(), df_meta_val[\"label\"].tolist(), albert_tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# Create DataLoaders for the meta learner\n",
        "bert_meta_train_loader = DataLoader(bert_meta_train_dataset, batch_size=32, shuffle=False)\n",
        "bert_meta_val_loader = DataLoader(bert_meta_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "roberta_meta_train_loader = DataLoader(roberta_meta_train_dataset, batch_size=32, shuffle=False)\n",
        "roberta_meta_val_loader = DataLoader(roberta_meta_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# deberta_train_loader = DataLoader(deberta_train_dataset, batch_size=64, shuffle=True)\n",
        "# deberta_val_loader = DataLoader(deberta_val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "albert_meta_train_loader = DataLoader(albert_meta_train_dataset, batch_size=32, shuffle=False)\n",
        "albert_meta_val_loader = DataLoader(albert_meta_val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "xz705LCBNHXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_predictions(model, dataloader):\n",
        "    \"\"\"Get softmax probabilities from the trained transformers so we can use them as input for the metalearner\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            probs = F.softmax(outputs.logits, dim=1)  # Convert logits to probabilities\n",
        "            all_preds.append(probs.cpu())\n",
        "\n",
        "    return torch.cat(all_preds, dim=0)  # Stack all predictions\n",
        "\n",
        "# Get softmax probabilities for the meta training set from the aready finetuned transformer models\n",
        "bert_preds = get_predictions(bert_model, bert_meta_train_loader)\n",
        "roberta_preds = get_predictions(roberta_model, roberta_meta_train_loader)\n",
        "albert_preds = get_predictions(albert_model, albert_meta_train_loader)\n",
        "\n",
        "# Combine predictions into a single tensor: shape [num_samples, num_models * num_classes], this is the input for the meta learner\n",
        "meta_inputs = torch.cat([bert_preds, roberta_preds, albert_preds], dim=1)\n",
        "meta_labels = torch.tensor(df_meta_train[\"label\"].values)\n",
        "\n",
        "# Create meta-learning dataset\n",
        "meta_dataset = torch.utils.data.TensorDataset(meta_inputs, meta_labels)\n",
        "meta_loader = DataLoader(meta_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BkNvySek5Bua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get softmax probabilities for validation set for the meta learner\n",
        "bert_val_preds = get_predictions(bert_model, bert_meta_val_loader)\n",
        "roberta_val_preds = get_predictions(roberta_model, roberta_meta_val_loader)\n",
        "albert_val_preds = get_predictions(albert_model, albert_meta_val_loader)\n",
        "\n",
        "# Combine into meta-validation inputs\n",
        "meta_val_inputs = torch.cat([bert_val_preds, roberta_val_preds, albert_val_preds], dim=1)\n",
        "meta_val_labels = torch.tensor(df_meta_val[\"label\"].values)\n",
        "\n",
        "# Create validation meta-learning dataset\n",
        "meta_val_dataset = torch.utils.data.TensorDataset(meta_val_inputs, meta_val_labels)\n",
        "meta_val_loader = DataLoader(meta_val_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "otiLD_0E5Dsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training fo the meta model\n",
        "meta_model = MetaLearner(input_size=meta_inputs.shape[1], num_classes=len(torch.unique(meta_labels)))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "meta_model.to(device)\n",
        "optimizer = torch.optim.Adam(meta_model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):  # It only overfits after 10 epochs\n",
        "    meta_model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in meta_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = meta_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(meta_loader)}\")\n",
        "\n",
        "#save the model for future use\n",
        "torch.save(meta_model.state_dict(), f\"{SAVE_PATH}/meta_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZBRtaPr5HV6",
        "outputId": "5acacaf6-5fda-4689-e5cf-d4f9d8191c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.39430740948985604\n",
            "Epoch 2, Loss: 0.3222089970813078\n",
            "Epoch 3, Loss: 0.32126476992579067\n",
            "Epoch 4, Loss: 0.32209871946012275\n",
            "Epoch 5, Loss: 0.3124090107048259\n",
            "Epoch 6, Loss: 0.31059635281562803\n",
            "Epoch 7, Loss: 0.3079418997554218\n",
            "Epoch 8, Loss: 0.3103067455922856\n",
            "Epoch 9, Loss: 0.30710761003634507\n",
            "Epoch 10, Loss: 0.30447167543803944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation of the meta model\n",
        "meta_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in meta_val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = meta_model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Meta-learning model accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln9tzI785JC8",
        "outputId": "3f9fd515-eab0-4aa7-e241-0033b9121998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-learning model accuracy: 0.8976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation of the meta model\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "meta_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in meta_val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = meta_model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Store predictions & labels for F1-score\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Convert lists to tensors\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "# Compute F1-score\n",
        "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "print(f\"Meta-learning model accuracy: {correct / total:.4f}\")\n",
        "print(f\"Meta-learning model F1-score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "# df_results = pd.DataFrame({\"prediction\": all_preds})\n",
        "# df_results.to_csv(\"meta_model_predictions.csv\", index=False)\n",
        "\n",
        "# print(\"Predictions saved to meta_model_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHmnY9oW5LPt",
        "outputId": "c9aa1c50-576c-4998-ca0e-3fa703bb985f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-learning model accuracy: 0.8976\n",
            "Meta-learning model F1-score: 0.8975\n",
            "Predictions saved to meta_model_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Getting results for the test dataset with no 'labels' column**"
      ],
      "metadata": {
        "id": "vOL6kMKXT0a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test dataset\n",
        "df_test = pd.read_csv(\"test.csv\", quotechar='\"', delimiter=\",\", encoding=\"utf-8\")\n",
        "df_test.columns = [\"premise\", \"hypothesis\"]  # Ensure correct column names\n",
        "\n",
        "\n",
        "# Apply text cleaning\n",
        "df_test[\"premise\"] = df_test[\"premise\"].apply(clean_text)\n",
        "df_test[\"hypothesis\"] = df_test[\"hypothesis\"].apply(clean_text)\n",
        "\n",
        "\n",
        "\n",
        "# Print first few samples\n",
        "print(\"Test Dataset Sample:\")\n",
        "print(df_test.head())\n",
        "print(f\"Test Dataset Shape: {df_test.shape}\")\n"
      ],
      "metadata": {
        "id": "fY-GA7qwAeWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#Dataset for the test set with no 'labels' column\n",
        "class NliTestDataset(Dataset):\n",
        "  \"\"\"Dataset for the NLI task\"\"\"\n",
        "  def __init__(self, premises, hypotheses, tokenizer, max_lenth = 124):\n",
        "    self.premises = premises\n",
        "    self.hypotheses = hypotheses\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_lenth\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.premises)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      encoding = self.tokenizer(self.premises[idx], self.hypotheses[idx], padding='max_length',truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "      return {\n",
        "          'input_ids': encoding['input_ids'].squeeze(0),\n",
        "          'attention_mask': encoding['attention_mask'].squeeze(0)\n",
        "      }\n",
        "\n",
        "# Initialize tokenizers for different models\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "# deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "albert_tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "#Create test datasets for each model\n",
        "bert_test_dataset = NliTestDataset(df_test[\"premise\"].tolist(), df_test[\"hypothesis\"].tolist(), bert_tokenizer)\n",
        "roberta_test_dataset = NliTestDataset(df_test[\"premise\"].tolist(), df_test[\"hypothesis\"].tolist(), roberta_tokenizer)\n",
        "albert_test_dataset = NliTestDataset(df_test[\"premise\"].tolist(), df_test[\"hypothesis\"].tolist(), albert_tokenizer)\n",
        "\n",
        "# Create Test DataLoaders\n",
        "bert_test_loader = DataLoader(bert_test_dataset, batch_size=32, shuffle=False)\n",
        "roberta_test_loader = DataLoader(roberta_test_dataset, batch_size=32, shuffle=False)\n",
        "albert_test_loader = DataLoader(albert_test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "deAk8VtY9zXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get predictions for the test data\n",
        "bert_test_preds = get_predictions(bert_model, bert_test_loader)\n",
        "roberta_test_preds = get_predictions(roberta_model, roberta_test_loader)\n",
        "albert_test_preds = get_predictions(albert_model, albert_test_loader)\n",
        "\n",
        "# Combine predictions into a single tensor: shape [num_samples, num_models * num_classes]\n",
        "meta_inputs = torch.cat([bert_test_preds, roberta_test_preds, albert_test_preds], dim=1)\n",
        "\n",
        "meta_test_dataset = torch.utils.data.TensorDataset(meta_inputs)\n",
        "meta_test_loader = DataLoader(meta_test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "XlxZMTIh-xCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "def get_meta_predictions(meta_model, test_loader, save_csv=False, csv_filename=\"meta_model_test_predictions.csv\"):\n",
        "    \"\"\"\n",
        "    Generates predictions for a given test dataset using the trained meta_model.\n",
        "\n",
        "    Args:\n",
        "    - meta_model: Trained PyTorch model\n",
        "    - test_loader: DataLoader for test dataset\n",
        "    - save_csv: Whether to save predictions as CSV (default: False)\n",
        "    - csv_filename: Name of the CSV file if saving (default: \"meta_model_test_predictions.csv\")\n",
        "\n",
        "    Returns:\n",
        "    - all_preds: Numpy array containing the predictions\n",
        "    \"\"\"\n",
        "    meta_model.eval()  # Set to evaluation mode\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in test_loader:  # No labels needed for test data\n",
        "            inputs = inputs[0].to(device)  # Extract inputs from DataLoader\n",
        "            outputs = meta_model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)  # Get class predictions\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    # Convert list of tensors to a single numpy array\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "\n",
        "    # Save predictions to CSV if required\n",
        "    if save_csv:\n",
        "        df_results = pd.DataFrame({\"prediction\": all_preds})\n",
        "        df_results.to_csv(csv_filename, index=False)\n",
        "        print(f\"Predictions saved to {csv_filename}\")\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "all_preds=[]\n",
        "all_preds = get_meta_predictions(meta_model, meta_test_loader, save_csv=True)\n"
      ],
      "metadata": {
        "id": "U-IG2Idn5NYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n"
      ],
      "metadata": {
        "id": "qu1phClw5O80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e4d2c1-138d-4ebb-baac-f0680bdbf2f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n",
            "Transformers version: 4.50.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2 --force-reinstall\n",
        "\n"
      ],
      "metadata": {
        "id": "GcucZgRC5Ruu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}